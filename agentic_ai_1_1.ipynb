{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Module 1 - Langchain Core\n",
        "## 1.1 Setup and LLMChain\n",
        "\n",
        "\n",
        "> Goals\n",
        "  1.   Install Langchain\n",
        "  2.   Use LCEL\n",
        "  3.   Test LCEL\n",
        "\n",
        "To install Langchain, use  ```pip install langchain```\n",
        "We will use llama with Groq.\n",
        "\n"
      ],
      "metadata": {
        "id": "bRGtsRJMTpC9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6imMXl-etM-_",
        "outputId": "db21c1d7-ea2a-48f2-da43-f3628964a4c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/130.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "pip install -qU \"langchain[groq]\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up environment\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.environ.get(\"GROQ_API_KEY\"):\n",
        "  os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"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\")\n",
        "\n",
        "# imorting dependencies\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# setting up llm,prompt template and chain\n",
        "model = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")\n",
        "prompt = ChatPromptTemplate.from_template(\"Translate this to French: {text}\")\n",
        "chain = prompt | model"
      ],
      "metadata": {
        "id": "YO3KyW-8Y1tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a chain to translate into french. Prompt template takes the promps and pass it through llm to get output."
      ],
      "metadata": {
        "id": "NhyQGGatZZ7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"text\": \"Hello, how are you?\"})"
      ],
      "metadata": {
        "id": "bus_AJ2VZRmF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e407a48-1bf3-459e-9532-fca3c338b65b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Bonjour, comment vas-tu?\\n\\n(Note: \"vas-tu\" is the informal way of saying \"how are you\" in French, used with friends or people you\\'re familiar with. If you want to use the formal way, you can say \"comment allez-vous?\")', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 58, 'prompt_tokens': 21, 'total_tokens': 79, 'completion_time': 0.128706761, 'prompt_time': 0.006431708, 'queue_time': 0.006583062999999999, 'total_time': 0.135138469}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_8b7c3a83f7', 'finish_reason': 'stop', 'logprobs': None}, id='run--8b31263d-9b32-4484-8fbd-b8631d243f35-0', usage_metadata={'input_tokens': 21, 'output_tokens': 58, 'total_tokens': 79})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us build LCEL chain that takes a product name and returns a company slogan."
      ],
      "metadata": {
        "id": "OeyPaaJbZoFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imorting dependencies\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# setting up llm,prompt template and chain\n",
        "model = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")\n",
        "prompt = ChatPromptTemplate.from_template(\"Write a company slogan for this {product}\")\n",
        "chain = prompt | model"
      ],
      "metadata": {
        "id": "N8O6YYaJZvR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.invoke({\"product\": \"eco water bottle\"})\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eO0T3rRQcM64",
        "outputId": "0e771c3a-f597-43d1-e0b9-c4389112e264"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Here are a few company slogan ideas for the eco water bottle:\\n\\n1. **\"Hydrate sustainably, every sip\"** - emphasizing the eco-friendly aspect of the product.\\n2. **\"Drink up, waste less\"** - highlighting the reduction of single-use plastic waste.\\n3. **\"Pure water, pure conscience\"** - appealing to customers who care about the environment.\\n4. **\"Sip, save, repeat\"** - a playful way to encourage customers to keep using the eco-friendly bottle.\\n5. **\"Quench your thirst, not the planet\\'s\"** - a clever phrase that resonates with environmentally conscious consumers.\\n6. **\"Bottle up the good stuff, not the waste\"** - emphasizing the benefits of using a reusable, eco-friendly water bottle.\\n7. **\"Elevate your hydration, elevate the earth\"** - a slogan that promotes a sense of responsibility and eco-friendliness.\\n8. **\"The sip that saves the planet, one bottle at a time\"** - a bold statement that encourages customers to make a positive impact.\\n\\nChoose the one that resonates with your brand\\'s values and messaging!' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 239, 'prompt_tokens': 19, 'total_tokens': 258, 'completion_time': 0.199166667, 'prompt_time': 0.003885723, 'queue_time': 0.067464507, 'total_time': 0.20305239}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'stop', 'logprobs': None} id='run--9072d3aa-5888-4787-8e41-3646c3b68534-0' usage_metadata={'input_tokens': 19, 'output_tokens': 239, 'total_tokens': 258}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Output parser\n",
        "Output parser is a technique to get a structured output. The basic syntax is ```StrOutputParser ``` .\n",
        " There are several more options  ```JsonOutputParser``` , ```PydanticOutputParser ```."
      ],
      "metadata": {
        "id": "Oenj9GNfrWFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imorting dependencies\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# setting up llm,prompt template,parser and chain\n",
        "model = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")\n",
        "prompt = ChatPromptTemplate.from_template(\"Write exactly one short company slogan for this {product}\")\n",
        "parser = StrOutputParser()\n",
        "chain = prompt | model |parser\n",
        "\n",
        "response = chain.invoke({\"product\": \"eco water bottle\"})\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UyaTPUiseQF",
        "outputId": "66438538-acc9-49b3-90c3-cc57015e372c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Sip Smart, Earth Happy\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Input\n",
        "Sometimes we have to process our input too. Users give inputs in a hapazhard way. On these cases we also have to modify inputs too.\n",
        "\n",
        "For these cases, we have to build function and implement it with ``` runnable ```.\n",
        "\n",
        "Now let us make a preprocessor which is also set our tone as catchy with product name. So we have to add anaother element tone with value catchy in our dictionary."
      ],
      "metadata": {
        "id": "vdEZ3lWe9TAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# defining function\n",
        "def add_tone(input : dict) -> dict:\n",
        "  input[\"tone\"]= {\"funny\"}\n",
        "  return input\n",
        "\n",
        "# building preprocessor with runnable\n",
        "preprocess = RunnableLambda(add_tone)\n",
        "\n",
        "# building propmpt and model\n",
        "model = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")\n",
        "prompt = ChatPromptTemplate.from_template(\"Write exactly one short company slogan for this {product} with a {tone} tone.\")\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# building chain\n",
        "chain = preprocess | prompt  |model | parser"
      ],
      "metadata": {
        "id": "4ZQsKj7THMEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "result = chain.invoke({\"product\": \"blood\"})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wnxvR13PoTh",
        "outputId": "f688e99f-383a-45ea-b978-ba76a553488e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Blood you can bank on... and also use to make a killer Bloody Mary\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding tools with LCEL\n",
        "To process different tasks we have to corporate more tools in our chain. Let us build a tool such that it will take weights in kg and return it in lbs. And then we will implement it with our chain."
      ],
      "metadata": {
        "id": "SXx71Hxs9WyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# defining weight tool\n",
        "def weight(input : dict) -> dict:\n",
        "  weight = input[\"weight\"]\n",
        "  weight_lbs = round(weight * 2.20462, 2)\n",
        "  return {\n",
        "      \"product\": input[\"product\"],\n",
        "      \"weight_in_lbs\": weight_lbs\n",
        "  }\n",
        "\n",
        "# building preprocessor with runnable\n",
        "weight_lbs = RunnableLambda(weight)\n",
        "\n",
        "# building propmpt and model\n",
        "model = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")\n",
        "prompt = ChatPromptTemplate.from_template(\"Write exactly one short company slogan for this {product} with a {weight_in_lbs} weight.\")\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# building chain\n",
        "chain = weight_lbs | prompt  |model | parser"
      ],
      "metadata": {
        "id": "MnAPiWSBaNbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "result = chain.invoke({\"product\": \"blood\",\"weight\" : 0})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kP-Zp_RYd4oi",
        "outputId": "75757b32-5e40-4aef-eb25-7a7ad9106a51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Sip, don't sin: Zero-Calorie Blood for a Clear Conscience\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building a Modular Multi-Step Chain\n",
        "So now we will differrent tools in one chain to do differrent work so that it we can logic in different chain in a multi-chain system.\n",
        "\n",
        "So we will build a small AI Village Assistant System.\n",
        "We will not use any agent. We will use LCEL only.\n",
        "\n",
        "So our chain will divide our query into three categories - symptom advice,nutritional tip and rest suggestions. We will need 4 chain - 1 classification chain and 3 chains for 3 types of outputs.\n"
      ],
      "metadata": {
        "id": "6ZRUaScuDpMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableBranch\n",
        "\n",
        "# classification chain\n",
        "model = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")\n",
        "prompt = ChatPromptTemplate.from_template(\"Classify input : {input} into exactly any one of these category- ['Symptom','Diet','Rest'].If you cannot classify input  return 'other'.Just return the category only.\")\n",
        "parser = StrOutputParser()\n",
        "classifier_chain = prompt | model | parser\n",
        "\n",
        "# Symptom tool\n",
        "sym_prompt = ChatPromptTemplate.from_template(\"You are one of the best medical consultant. Study the symptoms : {input} and give safe and hepful short advice. Do not include anything more.\")\n",
        "sym_chain = sym_prompt | model | parser\n",
        "\n",
        "# Diet tool\n",
        "diet_prompt = ChatPromptTemplate.from_template(\"You are one of the best dietician. Study the query:{input} and give compact short helpful nutritious diet advice only.\")\n",
        "diet_chain = diet_prompt | model | parser\n",
        "\n",
        "# Rest tool\n",
        "rest_prompt = ChatPromptTemplate.from_template(\"Based on users query:{input} give some helpful advices to rest.\")\n",
        "rest_chain = rest_prompt | model | parser\n",
        "\n",
        "# Fallback chain\n",
        "default_chain = RunnableLambda(lambda x: \"Sorry, I don't know how to respond to that.\")\n",
        "\n",
        "# branch chain\n",
        "branch_chain = RunnableBranch(\n",
        "    (lambda x:\"Symptom\" in x[\"type\"],sym_chain),\n",
        "    (lambda x:\"Diet\" in x[\"type\"],diet_chain),\n",
        "    (lambda x:\"Rest\" in x[\"type\"],rest_chain),\n",
        "    default_chain\n",
        "    )\n",
        "\n",
        "# combining all stages\n",
        "def classify_and_route(input:dict)->dict:\n",
        "  classification = classifier_chain.invoke({\"input\":input})\n",
        "\n",
        "  return branch_chain.invoke({\n",
        "      \"input\":input,\n",
        "      \"type\":classification.strip()\n",
        "  })"
      ],
      "metadata": {
        "id": "4861m_n4Dlcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_1 = classify_and_route({\"input\":\"I feel very cold and have headache\"})\n",
        "print(result_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfRAaJ9wfv5v",
        "outputId": "26f7d0cc-6c04-4368-dd26-3e85f6500423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the symptoms, it's possible that you may be experiencing a common cold or flu. Here's a short advice:\n",
            "\n",
            "* Drink plenty of warm liquids such as tea, coffee, or broth to help raise your body temperature and ease the cold sensation.\n",
            "* Take an over-the-counter pain reliever such as acetaminophen or ibuprofen to help alleviate your headache.\n",
            "* Rest and avoid strenuous activities to help your body recover.\n",
            "\n",
            "Please consult a healthcare professional if your symptoms worsen or persist.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_2 = classify_and_route({\"input\":\"What food can I eat during weakness?\"})\n",
        "print(result_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQtTXOnggZYm",
        "outputId": "d45305cf-c21f-4243-e7fd-fc4a34782e4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thank you for reaching out! When you're feeling weak, it's essential to fuel up with nutrient-rich foods that can help boost your energy levels. Here are some compact and helpful advice:\n",
            "\n",
            "**Eat these foods:**\n",
            "\n",
            "1. **Bananas**: Rich in potassium, bananas help regulate fluid balance and electrolytes, which can help alleviate fatigue.\n",
            "2. **Leafy Greens**: Spinach, kale, and collard greens are packed with iron, which is essential for healthy red blood cells and energy production.\n",
            "3. **Nuts and Seeds**: Almonds, cashews, pumpkin seeds, and sunflower seeds are rich in healthy fats, protein, and fiber, which can help stabilize blood sugar levels and provide a energy boost.\n",
            "4. **Fatty Fish**: Fatty fish like salmon, tuna, and mackerel are rich in omega-3 fatty acids, which can help reduce inflammation and support energy production.\n",
            "5. **Sweet Potatoes**: Rich in complex carbohydrates, sweet potatoes provide sustained energy and are easy to digest.\n",
            "6. **Avocados**: Avocados are a rich source of healthy fats, fiber, and various vitamins and minerals, which can help support energy production and overall health.\n",
            "7. **Greek Yogurt**: Greek yogurt is an excellent source of protein, which can help stabilize blood sugar levels and provide a feeling of fullness and satisfaction.\n",
            "\n",
            "**Remember to:**\n",
            "\n",
            "* Drink plenty of water to stay hydrated\n",
            "* Avoid sugary and processed foods that can cause energy crashes\n",
            "* Incorporate these foods into your meals and snacks to help stabilize your energy levels\n",
            "\n",
            "By incorporating these foods and tips into your diet, you'll be well on your way to boosting your energy levels and feeling your best!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_3 = classify_and_route({\"input\":\"My sleep is disturbed due to stress\"})\n",
        "print(result_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeAs_Gkbgh53",
        "outputId": "e49ed402-91c4-4b04-8ea7-a69110ba224f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the symptom \"My sleep is disturbed due to stress\", I would advise:\n",
            "\n",
            "\"Take 10-15 minutes each day to practice relaxation techniques such as deep breathing, progressive muscle relaxation, or guided imagery to help calm your mind and body. Establish a consistent sleep schedule and create a bedtime routine to signal your body that it's time to sleep. Avoid screens and stimulating activities at least an hour before bedtime. Consider keeping a stress journal to identify and manage stressors in your life.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_4 = classify_and_route({\"input\":\"djbnfjdjsbjjaduhufjdkjjzm md mz mmmmmmmmmmsmmmmm\"})\n",
        "print(result_4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyE8FGojg0UP",
        "outputId": "e56d67f8-a1d7-4be0-a395-1240cf8185f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry, I don't know how to respond to that.\n"
          ]
        }
      ]
    }
  ]
}